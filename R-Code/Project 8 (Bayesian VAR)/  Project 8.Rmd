---
title: 'Project 1: Big Data'
author: "David Contento"
date: "May 1, 2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
setwd("C:/Users/David/Desktop/Grad school work/412/HW1")
#remotes::install_github("franzmohr/bvartools")
library(bvartools)
data_SP = read.csv("SP.csv")
data_GE = read.csv("GE.csv")
```

We have daily stock data from Yahoo Finance dating back to January 2nd, 1962 for the S&P 500 and General Electric. We chose General Electric because it has a large amount of data and likely moves closely with the S&P 500 (i.e Market beta close to 1).
```{r}
#running CAPM to determine market beta
library(readxl)
ff=read_excel("FF3F.xlsx")
gemonth=read.csv("GEmonth.csv")
ff=ff[427:1112,]
ff=ff[427:1112,]
gemonth=gemonth[1:686,]
gemonth$return=0
for(i in 2:686){
  gemonth$return[i]=(((gemonth$Close[i]-gemonth$Close[i-1])/gemonth$Close[i-1])*100)
}
ff$return=gemonth$return
ff=ff[-1,]
ff$newreturn=ff$return-ff$RF
#CAPM
reg1=lm(ff$newreturn~ff$`Mkt-RF`)
summary(reg1)
```

From the output above we can see that the market beta for GE stock is close to one, which signifies that the stock does not over or underreact to movements in the market.

```{r}
#converting to time series and creating log difference 
data=data.frame(data_GE$Close,data_SP$Close)
data_ts=ts(data, start=1, frequency = 1)

plot(data_ts, main="Time Series Data for GE and S&P 500")
```

We notice that the data is not stationary so we take the log difference of the data.
```{r}

e1 <- diff(log(data_ts))

#plotting log difference 
plot(e1, main="Log Differenced Data for GE & S&P 500")

```

Now we will set up our parameters to run the Bayesian Vector Autoregression. We use a uniform prior.

```{r}

#creating matrix for regressors and independent variables
data_input <- gen_var(e1, p = 4)
y <- data_input$Y[, 1:73]
x <- data_input$Z[, 1:73]
```


```{r}
#setting up BVAR and gibbs sampler
set.seed(1234567)

iter <- 15000 # Number of iterations of the Gibbs sampler
burnin <- 5000 # Number of burn-in draws
store <- iter - burnin

t <- ncol(y) # Number of observations
k <- nrow(y) # Number of endogenous variables
m <- k * nrow(x) # Number of estimated coefficients

# Set (uninformative) priors
a_mu_prior <- matrix(0, m) # Vector of prior parameter means
a_v_i_prior <- diag(0, m) # Inverse of the prior covariance matrix

u_sigma_df_prior <- 0 # Prior degrees of freedom
u_sigma_scale_prior <- diag(0, k) # Prior covariance matrix
u_sigma_df_post <- t + u_sigma_df_prior # Posterior degrees of freedom

# Initial values
u_sigma_i <- diag(.00001, k)
u_sigma <- solve(u_sigma_i)

# Data containers for posterior draws
draws_a <- matrix(NA, m, store)
draws_sigma <- matrix(NA, k^2, store)

```

We use the Gibbs Sampler to make draws from the data, using 4 lags.
```{r}
# Start Gibbs sampler
for (draw in 1:iter) {
  # Draw conditional mean parameters
  a <- post_normal(y, x, u_sigma_i, a_mu_prior, a_v_i_prior)
  
  # Draw variance-covariance matrix
  u <- y - matrix(a, k) %*% x # Obtain residuals
  u_sigma_scale_post <- solve(u_sigma_scale_prior + tcrossprod(u))
  u_sigma_i <- matrix(rWishart(1, u_sigma_df_post, u_sigma_scale_post)[,, 1], k)
  u_sigma <- solve(u_sigma_i) # Invert Sigma_i to obtain Sigma
  
  # Store draws
  if (draw > burnin) {
    draws_a[, draw - burnin] <- a
    draws_sigma[, draw - burnin] <- u_sigma
}
}
```

```{r,include=FALSE}

A <- rowMeans(draws_a) # Obtain means for every row
A <- matrix(A, k) # Transform mean vector into a matrix
A <- round(A, 3) # Round values
dimnames(A) <- list(dimnames(y)[[1]], dimnames(x)[[1]]) # Rename matrix dimensions
#A # Print


Sigma <- rowMeans(draws_sigma) # Obtain means for every row
Sigma <- matrix(Sigma, k) # Transform mean vector into a matrix
Sigma <- round(Sigma * 10^4, 2) # Round values
dimnames(Sigma) <- list(dimnames(y)[[1]], dimnames(y)[[1]]) # Rename matrix dimensions
#Sigma # Print
```

Below, we use bvar to create estimates from samples.
```{r}

#creating bvar estimates
bvar_est <- bvar(y = y, x = x, A = draws_a[1:10,],
                 C = draws_a[11:18,], Sigma = draws_sigma)


#bvar_pred <- predict(bvar_est, n.ahead = 30, new_D = rep(1,30))

#plot(bvar_pred)
```

Given these estimates, we can see that a shock in S&P 500 has a minimal effect on GE, due to the IR converging to zero quickly. Conversely we notice that a shock in GE has no real effect on the S&P 500 as the scale for the response is really small.


```{r}
#Impulse response function and plot 
IR <- irf(bvar_est, impulse = "data_SP.Close", response = "data_GE.Close", n.ahead = 8)
plot(IR, main = "Impulse Response for a Shock in S&P 500 on GE", xlab = "Period", ylab = "Response")

#Impulse response function and plot 
IR <- irf(bvar_est, impulse = "data_GE.Close", response = "data_SP.Close", n.ahead = 8)
plot(IR, main = "Impulse Response for a Shock in GE on S&P 500", xlab = "Period", ylab = "Response")
```

Below, we see that a shock in GE can explain about 40% of the forecast error variance in S&P 500. While, vice versa, as shock in the S&P 500 can explain almost 100% of the forecast error variance in the S&P 500. 

```{r}
#Forecasted errors variance decomposition.
bvar_fevd <- fevd(bvar_est, response = "data_SP.Close")
bvar_fevd1 <- fevd(bvar_est, response = "data_GE.Close")

plot(bvar_fevd, main = "FEVD of GE on S&P 500")
plot(bvar_fevd1, main = "FEVD of S&P on GE")
```
Next we look at the granger causality over time.
```{r,message=F,warning=F}
#determining granger causality over time
library(lmtest)
grangertest(data_GE$Close~data_SP$Close)
grangertest(data_GE$Close~data_SP$Close)

granger1=NULL
spdata=data_SP$Close[0:1000]
gedata=data_GE$Close[0:1000]

for(i in 1:2689){
  #print(i)
  j=grangertest(gedata~spdata)
  if(j$`Pr(>F)`[2]>0.05){
    granger1[i]=1
    
  }else{
    granger1[i]=0
  }
  spdata=data_SP$Close[0:length(spdata)+5]
  gedata=data_GE$Close[0:length(gedata)+5]
}

granger2=NULL
spdata=data_SP$Close[0:1000]
gedata=data_GE$Close[0:1000]
for(i in 1:2689){
  #print(i)
  j=grangertest(spdata~gedata)
  if(j$`Pr(>F)`[2]>0.05){
    granger2[i]=1
    
  }else{
    granger2[i]=0
  }
  spdata=data_SP$Close[0:length(spdata)+5]
  gedata=data_GE$Close[0:length(gedata)+5]
}

#percentage of time S&P 500 is not granger causing GE
mean(granger1)


#percentage of time GE is not granger causing S&P 500
mean(granger2)
```
From this, we see that GE is never granger causing the S&P 500. However, given that the mean granger test is less than 1 for the S&P 500 on GE, we can conclude that over time the S&P 500 is more likely to granger cause GE.















